{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully received response.\n",
      "Response JSON: {'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzE4NDQ4NzI3LjAxOTc2LCJpYXQiOjE3MTgzNjIzMjcuMDE5NzYsImp0aSI6Ikl1N0I4YjhPU1RWbXJzaE9ZdFBpMHQ1RmlFNlJhUSIsImNpZCI6IjZFSFdsNjFTTTNGdEE3M2tyeUMzQ2ciLCJsaWQiOiJ0Ml8xMmc3a3gyZDZvIiwiYWlkIjoidDJfMTJnN2t4MmQ2byIsImxjYSI6MTcxODM0MzgxMjA3Nywic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.N1UU5yeRlgod4ZqFKKK-L-vM3DrDBi8vwR_lRRGW-sYe5ZzmVzUXlFzHNWtjoKmuOdM5UtaD3qnTiqrOPihMP1qkqv_y1DiMSl6jCQt_rZGlT1Ng79tMrd9kHqzglchlYu559If4uv2RbkgInhc5s00p6OJaDw6FH2JGW4lV5e3y5Voppl4hftInnpfZWUauU4sQes9E8u26S73-MXPfAoT-HeaWenqETMr4Oz-2zVT0tvmCseCka0zS9HWnC0x9gc81dCxFLOOfVoE4E57RriedFaIDiPvBqpvc-jJW-Pvt0HvXFieN1ppbRkKVGSD-MgRO6SLnM9cI6jfxFAzfyA', 'token_type': 'bearer', 'expires_in': 86400, 'scope': '*'}\n",
      "Successfully retrieved data for subreddit: bespoke\n",
      "Successfully retrieved data for subreddit: fashion\n",
      "Successfully retrieved data for subreddit: B2B\n",
      "Successfully retrieved data for subreddit: design\n",
      "Successfully retrieved data for subreddit: 3D\n",
      "Successfully retrieved data for subreddit: Visualization\n",
      "Successfully retrieved data for subreddit: Shopify\n",
      "Successfully retrieved data for subreddit: PageBuilder\n",
      "Successfully retrieved data for subreddit: Product\n",
      "Successfully retrieved data for subreddit: Visualstore\n",
      "Successfully retrieved data for subreddit: Threejs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def authenticate():\n",
    "    base_url = 'https://www.reddit.com/'\n",
    "    data = {\n",
    "        'grant_type': 'password', \n",
    "        'username': 'Coder2108', \n",
    "        'password': '#12345678'\n",
    "    }\n",
    "    auth = requests.auth.HTTPBasicAuth('6EHWl61SM3FtA73kryC3Cg', 'ivpZJVmNdmOWF_D4LziJlAWg9mjm7Q')\n",
    "    headers = {'User-Agent': 'Scrap'}\n",
    "    \n",
    "    response = requests.post(base_url + 'api/v1/access_token', data=data, headers=headers, auth=auth)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully received response.\")\n",
    "        response_json = response.json()\n",
    "        print(\"Response JSON:\", response_json) \n",
    "        return response_json.get('access_token')  \n",
    "    else:\n",
    "        print(f\"Failed to get response: {response.status_code}\")\n",
    "        return None \n",
    "\n",
    "\n",
    "def fetch_comments(post_id, token):\n",
    "    base_url = 'https://oauth.reddit.com'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': 'Scrap'\n",
    "    }\n",
    "\n",
    "    url = f'{base_url}/comments/{post_id}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        comments_data = response.json()\n",
    "        comments_list = []\n",
    "\n",
    "      \n",
    "        for comment in comments_data[1]['data']['children']:  \n",
    "            if 'body' in comment['data']: \n",
    "                comments_list.append(comment['data'])\n",
    "\n",
    "   \n",
    "      \n",
    "    \n",
    "def fetch_data(token, subreddits, keywords):\n",
    "    base_url = 'https://oauth.reddit.com'\n",
    "    headers = {\n",
    "        'Authorization': f'bearer {token}',\n",
    "        'User-Agent': 'Scrap'\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        search_keywords = keywords + [keyword.lower() for keyword in keywords]\n",
    "        search_query = f'subreddit:{subreddit} ({ \" OR \".join(search_keywords) })'\n",
    "        url = f'{base_url}/search?q={search_query}&limit=10'\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Successfully retrieved data for subreddit: {subreddit}\")\n",
    "            response_json = response.json()\n",
    "            posts = response_json['data']['children']\n",
    "\n",
    "            for post in posts:\n",
    "                post_data = post['data']\n",
    "                title = post_data['title']\n",
    "                post_id = post_data['id']\n",
    "                post_url = f\"https://www.reddit.com{post_data['permalink']}\"\n",
    "\n",
    "                \n",
    "                comments = fetch_comments(post_id, token)\n",
    "\n",
    "                data.append({\n",
    "                    'subreddit': subreddit,\n",
    "                    'url': post_url,\n",
    "                    'title': title,\n",
    "                    'comments': comments,\n",
    "                    'keywords': keywords\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for subreddit: {subreddit}. Status code: {response.status_code}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_to_csv(data, filename='reddit_data.csv'):\n",
    "    keys = ['subreddit', 'url', 'title', 'comments', 'keywords']\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for entry in data:\n",
    "            \n",
    "            comments_str = ' | '.join(entry['comments']) if entry['comments'] else ''\n",
    "            \n",
    "            \n",
    "            writer.writerow({\n",
    "                'subreddit': entry['subreddit'],\n",
    "                'url': entry['url'],\n",
    "                'title': entry['title'],\n",
    "                'comments': comments_str,\n",
    "                'keywords': ', '.join(entry['keywords'])\n",
    "            })\n",
    "\n",
    "\n",
    "def main():\n",
    "    token = authenticate()\n",
    "    \n",
    "    if token:\n",
    "        subreddits = ['bespoke', 'fashion', 'B2B', 'design', '3D', 'Visualization', 'Shopify', 'PageBuilder', 'Product', 'Visualstore', 'Threejs']\n",
    "        keywords = [\n",
    "            'Personalization',\n",
    "            'Fashion',\n",
    "            'Visulisation',\n",
    "            'Bespoke',\n",
    "            'Business',\n",
    "            'Studio',\n",
    "            'Customization',\n",
    "            '3D customization',\n",
    "            '3Dcustomization',\n",
    "            'B2b'\n",
    "        ]\n",
    "\n",
    "        data = fetch_data(token, subreddits, keywords)\n",
    "        \n",
    "        \n",
    "        write_to_csv(data, filename='reddit_data.csv')\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
